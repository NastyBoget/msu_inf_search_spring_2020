{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import re\n",
    "from multiprocessing import Pool, Lock, Value\n",
    "from time import sleep\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pymystem3 import Mystem\n",
    "from _collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = pd.read_csv('queries.numerate.txt', sep='\t', header=None)\n",
    "urls = pd.read_csv('urls.numerate.txt', sep='\t', header=None)\n",
    "samples = pd.read_csv('sample.technosphere.ir1.textrelevance.submission.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentName</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DocumentId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170707/doc.2351.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170707/doc.2661.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170707/doc.1883.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170707/doc.0713.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20170707/doc.0996.dat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     DocumentName\n",
       "DocumentId                       \n",
       "1           20170707/doc.2351.dat\n",
       "2           20170707/doc.2661.dat\n",
       "3           20170707/doc.1883.dat\n",
       "4           20170707/doc.0713.dat\n",
       "5           20170707/doc.0996.dat"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docid_df = pd.read_csv('docids.txt')\n",
    "docid_df.index = docid_df['DocumentId']\n",
    "docid_df = docid_df.drop(columns=['DocumentId'])\n",
    "docid_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = Mystem()\n",
    "PATTERN = re.compile(r'[A-Za-zА-Яа-я0-9]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2words(doc_id):\n",
    "    with open('content/content/' + docid_df.iloc[doc_id - 1]['DocumentName'], errors='ignore') as read_file:\n",
    "        lines = list(read_file)\n",
    "    html = \"\".join(lines[1:])\n",
    "    soup = BeautifulSoup(html)\n",
    "    [s.extract() for s in soup(['script', 'style', 'title', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]\n",
    "    body = soup.get_text('\\n', True).lower()\n",
    "    body = PATTERN.findall(body)\n",
    "    body = ' '.join([stem.lemmatize(word)[0] for word in body])\n",
    "    \n",
    "    soup = BeautifulSoup(html)\n",
    "    title = ' '.join(e.get_text() for e in soup.find_all('title')).lower()\n",
    "    title = PATTERN.findall(title)\n",
    "    title = ' '.join([stem.lemmatize(word)[0] for word in title])\n",
    "      \n",
    "    headers = ' '.join([e.get_text() for e in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]).lower()\n",
    "    headers = PATTERN.findall(headers)\n",
    "    headers = ' '.join([stem.lemmatize(word)[0] for word in headers])\n",
    "\n",
    "    with open('parsed/{}.txt'.format(doc_id), 'w') as f:\n",
    "        f.write(title + '\\n')\n",
    "        f.write(headers + '\\n')\n",
    "        f.write(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38110 objects are processed..."
     ]
    }
   ],
   "source": [
    "mutex = Lock()\n",
    "n_processed = Value('i', 0)\n",
    "\n",
    "def func_wrapper(doc_id):\n",
    "    doc2words(doc_id) \n",
    "    with mutex:\n",
    "        # в этом блоке можно безопасно менять общие объекты для процессов\n",
    "        global n_processed\n",
    "        n_processed.value += 1\n",
    "        if n_processed.value % 10 == 0:\n",
    "            print(f\"\\r{n_processed.value} objects are processed...\", end='', flush=True)\n",
    "    \n",
    "with Pool(processes=12) as pool:\n",
    "    pool.map(func_wrapper, docid_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/f9d93756035e66c406a96470c7bf801b5161e238\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/c652b6871ce4872c8e924ff0f806bc8b06dc94ed\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(q_{i},D)$ — частота слова (term frequency, TF) $q_{i}$ в документе D\n",
    "\n",
    "$|D|$ — длина документа (количество слов в нём)\n",
    "\n",
    "$avgdl$ — средняя длина документа в коллекции\n",
    "\n",
    "$k_{1}$ и $b$ — cвободные коэффициенты, обычно их выбирают как $k_{1}=2.0$ и $b=0.75$\n",
    "\n",
    "$N$ — общее количество документов в коллекции\n",
    "\n",
    "$n(q_{i})$ — количество документов, содержащих $q_{i}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2docs = defaultdict(list)\n",
    "\n",
    "for _, text in queries.iterrows():\n",
    "    query_id = text[0]\n",
    "    rows_df = samples.loc[samples['QueryId'] == query_id]\n",
    "    for row in rows_df.iterrows():\n",
    "        doc_id = row[1]['DocumentId']\n",
    "        query2docs[query_id].append(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocStats:\n",
    "    def __init__(self):\n",
    "        self.collection_len = 0\n",
    "        self.title_len = 0\n",
    "        self.headers_len = 0\n",
    "        self.body_len = 0\n",
    "        self.title2idf = defaultdict(float)\n",
    "        self.headers2idf = defaultdict(float)\n",
    "        self.body2idf = defaultdict(float)\n",
    "        for docs in tqdm(query2docs.values()):\n",
    "            for doc_id in docs:\n",
    "                with open('parsed/{}.txt'.format(doc_id), errors='ignore') as f:\n",
    "                    self.collection_len += 1\n",
    "                    title = PATTERN.findall(f.readline().lower())\n",
    "                    self.title_len += len(title)\n",
    "                    self.wordStats(title, self.title2idf)\n",
    "                    headers = PATTERN.findall(f.readline().lower())\n",
    "                    self.headers_len += len(headers)\n",
    "                    self.wordStats(headers, self.headers2idf)\n",
    "                    body = PATTERN.findall(f.read().lower())\n",
    "                    self.body_len += len(body)\n",
    "                    self.wordStats(body, self.body2idf)\n",
    "        self.countIDF(self.title2idf)\n",
    "        self.countIDF(self.headers2idf)\n",
    "        self.countIDF(self.body2idf)\n",
    "        self.title_len /= self.collection_len\n",
    "        self.headers_len /= self.collection_len\n",
    "        self.body_len /= self.collection_len\n",
    "        \n",
    "    def wordStats(self, text, IDFdict):\n",
    "        processed = set()\n",
    "        for word in text:\n",
    "            if word not in processed:\n",
    "                IDFdict[word] += 1\n",
    "                processed.add(word)\n",
    "                \n",
    "    def countIDF(self, IDFdict):\n",
    "        idf_sum = 0\n",
    "        neg_idf = []\n",
    "        for word in IDFdict:\n",
    "            idf = np.log(self.collection_len - IDFdict[word] + 0.5) - np.log(IDFdict[word] + 0.5)\n",
    "            IDFdict[word] = idf\n",
    "            idf_sum += idf\n",
    "            if idf < 0:\n",
    "                neg_idf.append(word)\n",
    "        eps = 0.25 * idf_sum / len(IDFdict)\n",
    "        for word in neg_idf:\n",
    "            IDFdict[word] = eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25:\n",
    "    def __init__(self, k=2.0, b=0.75):\n",
    "        self.k = k\n",
    "        self.b = b\n",
    "        self.doc_stats = DocStats()        \n",
    "\n",
    "    def getBest(self, query_id):\n",
    "        doc2score = {}\n",
    "        for doc_id in query2docs[query_id]:\n",
    "            doc2score[doc_id] = self.getScore(query_id, doc_id)\n",
    "        doc2score = sorted(doc2score.items(), key=lambda x: x[1], reverse=True)\n",
    "        return doc2score\n",
    "\n",
    "    def getScore(self, query_id, doc_id):\n",
    "        title_tf = defaultdict(float)\n",
    "        headers_tf = defaultdict(float)\n",
    "        body_tf = defaultdict(float)\n",
    "        with open('parsed/{}.txt'.format(doc_id), errors='ignore') as f:\n",
    "            title = PATTERN.findall(f.readline().lower())\n",
    "            headers = PATTERN.findall(f.readline().lower())\n",
    "            body = PATTERN.findall(f.read().lower())\n",
    "            title_len = len(title)\n",
    "            headers_len = len(headers)\n",
    "            body_len = len(body)\n",
    "            for word in title:\n",
    "                title_tf[word] += 1\n",
    "            for word in headers:\n",
    "                headers_tf[word] += 1\n",
    "            for word in body:\n",
    "                body_tf[word] += 1\n",
    "        \n",
    "        title_score = 0\n",
    "        headers_score = 0\n",
    "        body_score = 0\n",
    "        \n",
    "        query = queries.iloc[query_id - 1][1]\n",
    "        query = PATTERN.findall(' '.join(stem.lemmatize(query.lower())))\n",
    "\n",
    "        for word in query:\n",
    "            title_score += self.doc_stats.title2idf[word] * (title_tf[word] * (self.k + 1)) / \\\n",
    "                    (title_tf[word] + self.k * (1 - self.b + self.b * title_len / self.doc_stats.title_len))\n",
    "            headers_score += self.doc_stats.headers2idf[word] * (headers_tf[word] * (self.k + 1)) / \\\n",
    "                    (headers_tf[word] + self.k * (1 - self.b + self.b * headers_len / self.doc_stats.headers_len))\n",
    "            body_score += self.doc_stats.body2idf[word] * (body_tf[word] * (self.k + 1)) / \\\n",
    "                    (body_tf[word] + self.k * (1 - self.b + self.b * body_len / self.doc_stats.body_len))\n",
    "        return 2.7 * title_score + 1.0 * headers_score + 1.5 * body_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 399/399 [02:54<00:00,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "model = BM25()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 399/399 [03:19<00:00,  2.00it/s]\n"
     ]
    }
   ],
   "source": [
    "QueryId = []\n",
    "\n",
    "docsId = []\n",
    "for query_id in tqdm(range(1, 400)):\n",
    "    best = model.getBest(query_id)\n",
    "    for item in best:\n",
    "        QueryId.append(query_id)\n",
    "        docsId.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({\n",
    "    'QueryId' : QueryId,\n",
    "    'DocumentId' : docsId\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QueryId</th>\n",
       "      <th>DocumentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   QueryId  DocumentId\n",
       "0        1          78\n",
       "1        1          28\n",
       "2        1          51\n",
       "3        1          53\n",
       "4        1          83"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('result.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
