{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import re\n",
    "from multiprocessing import Pool, Lock, Value\n",
    "from time import sleep\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = pd.read_csv('queries.numerate.txt', sep='\t', header=None)\n",
    "urls = pd.read_csv('urls.numerate.txt', sep='\t', header=None)\n",
    "samples = pd.read_csv('sample.technosphere.ir1.textrelevance.submission.txt')\n",
    "urls.index = urls[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docnames = []\n",
    "\n",
    "listdir = os.listdir('content/content/')\n",
    "\n",
    "for d in listdir:\n",
    "    listdocs = os.listdir('content/content/' + d)\n",
    "    listdocs.sort()\n",
    "    docnames += list(map(lambda x: d + '/' + x, listdocs))\n",
    "docnames[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('content/content/' + docnames[0], errors='ignore') as read_file:\n",
    "    lines = list(read_file)\n",
    "html = \"\".join(lines[1:])\n",
    "soup = BeautifulSoup(html)\n",
    "soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# doc_name | doc_id\n",
    "\n",
    "doc_ids = []\n",
    "\n",
    "for docname in docnames:\n",
    "    with open('content/content/' + docname, errors='ignore') as read_file:\n",
    "        lines = list(read_file)\n",
    "    url = lines[0].strip()\n",
    "    doc_id = urls.at[url, 0]\n",
    "    doc_ids.append(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_df = pd.DataFrame({\n",
    "    'DocumentName' : docnames,\n",
    "    'DocumentId' : doc_ids\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_df = docid_df.sort_values('DocumentId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_df.to_csv('docids.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_df = pd.read_csv('docids.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_df.index = docid_df['DocumentId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_df = docid_df.drop(columns=['DocumentId', 'DocumentId.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_df.iloc[0]['DocumentName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2words(doc_id):\n",
    "    with open('content/content/' + docid_df.iloc[doc_id]['DocumentName'], errors='ignore') as read_file:\n",
    "        lines = list(read_file)\n",
    "    html = \"\".join(lines[1:])\n",
    "    soup = BeautifulSoup(html)\n",
    "    title = \"\"\n",
    "    for a in soup.find_all('a'):\n",
    "        if a.get('title'):\n",
    "            title += ' '\n",
    "            title += re.sub(r'[^A-Za-zА-Яа-я0-9]+', ' ', a['title'])\n",
    "    if soup.title:\n",
    "        if soup.title.text:\n",
    "            title += ' '\n",
    "            title += re.sub(r'[^A-Za-zА-Яа-я0-9]+', ' ', soup.title.text)\n",
    "    if soup.text:\n",
    "        title += ' '\n",
    "        title += re.sub(r'[^A-Za-zА-Яа-я0-9]+', ' ', soup.text)\n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### index + tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stop_words.txt', \"r\") as rf:\n",
    "    lines = [line.strip() for line in rf]\n",
    "stop_words = set(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{term1: [[doc_id1, term_freq1], [doc_id2, term_freq2], ...], term2: ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "# for id, doc in tqdm(doc_df.iterrows(), total=38114, position=0):\n",
    "def create_index(doc_id):\n",
    "    text = doc2words(doc_id)\n",
    "    loc_index = {}\n",
    "    terms = str(text).split()\n",
    "    terms = [t.lower().strip() for t in terms]\n",
    "    terms = [t for t in terms if not t in stop_words]\n",
    "    terms = [stemmer.stem(term) for term in terms]\n",
    "    for term in terms:\n",
    "        if term in loc_index.keys():\n",
    "            doc_id, num = loc_index[term][-1]\n",
    "            if doc_id == id:\n",
    "                loc_index[term][-1][1] += 1\n",
    "            else:\n",
    "                loc_index[term].append([id, 1])\n",
    "        else:\n",
    "            loc_index[term] = [[id, 1]]\n",
    "            \n",
    "    # computing tf\n",
    "    for term in terms:\n",
    "        loc_index[term][-1][1] /= len(terms)\n",
    "    return loc_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### поисковые расширения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonims = {\n",
    "    \"применить\": [\"использовать\"],\n",
    "    \"инстаграм\": [\"instagram\"],\n",
    "    \"кап\": [\"капитальный\"],\n",
    "    \"биос\": [\"bios\"],\n",
    "    \"майнкрафт\": [\"minecraft\"],\n",
    "    \"авто\": [\"автомобиль\", \"машина\"],\n",
    "    \"гта\": [\"gta\"],\n",
    "    \"опфр\": [\"пенсионный\", \"фонд\", \"российской\", \"федерации\"],\n",
    "    \"ифнс\": [\"инспекция\", \"федеральной\", \"налоговой\", \"службы\"],\n",
    "    \"бесишь\": [\"раздражаешь\", \"злишь\", \"нервируешь\"],\n",
    "    \"вай\": [\"wi\"],\n",
    "    \"фай\": [\"fi\"],\n",
    "    \"соц\": [\"социальный\"],\n",
    "    \"вк\": [\"vk\", \"vkontakte\", \"вконтакте\"],\n",
    "    \"кс\": [\"cs\", \"counter\", \"strike\"],\n",
    "    \"дискорд\": [\"discord\"],\n",
    "    \"киви\": [\"kiwi\"],\n",
    "    \"трейнз\": [\"trainz\"],\n",
    "    \"мерседес\": [\"mercedes\"],\n",
    "    \"симс\": [\"sims\"],\n",
    "    \"биос\": [\"bios\"],\n",
    "    \"псп\": [\"playstation\" \"portable\", \"psp\"],\n",
    "    \"мод\": [\"mode\"],\n",
    "    \"одн\": [\"общедомовые\", \"нужды\"],\n",
    "    \"мегафон\": [\"megafon\"],\n",
    "    \"асти\": [\"asti\"],\n",
    "    \"пдф\": [\"pdf\"],\n",
    "    \"бмп\": [\"bmp\"], \n",
    "    \"ммр\": [\"mmr\"],\n",
    "    \"ккал\": [\"калория\"],\n",
    "    \"поу\": [\"pou\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QueryId = []\n",
    "\n",
    "for i in range(1, 400):\n",
    "    QueryId += [i] * 10\n",
    "    \n",
    "# size = 10 * 399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* term-frequency\n",
    "\n",
    "$tf = \\frac{n_t}{\\sum{n_k}}$\n",
    "\n",
    "$n_t$ - число вхождений слова t в документ\n",
    "\n",
    "$\\sum{n_k}$ - общее число слов в данном документе\n",
    "\n",
    "* inverse document frequency\n",
    "\n",
    "$idf = log \\frac{|D|}{|d_i : t \\in d_i|}$ \n",
    "\n",
    "$|D|$ - число документов в коллекции\n",
    "\n",
    "$|d_i : t \\in d_i|$ - число документов из коллекции D, в которых встречается t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text[0] - QueryId\n",
    "\n",
    "text[1] - queries text\n",
    "\n",
    "docs_list = [[doc_id1, term_freq1], [doc_id2, term_freq2], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"russian\") \n",
    "\n",
    "def get_prediction(query_row):\n",
    "    id, text = query_row\n",
    "    rows_df = samples.loc[samples['QueryId'] == id]\n",
    "    D = rows_df.shape[0]\n",
    "    res = []\n",
    "    for row in rows_df.iterrows():\n",
    "        doc_id = row[1]['DocumentId']\n",
    "        local_index = create_index(doc_id)\n",
    "        res.append(local_index)\n",
    "    # unite  indexes\n",
    "    index = {}\n",
    "    for i in res:\n",
    "        for key, value in i.items():\n",
    "            if key in index:\n",
    "                index[key] += value\n",
    "                \n",
    "            else:\n",
    "                index[key] = value\n",
    "    words = text[1].split()\n",
    "    words = [w.lower().strip() for w in words]\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    add = []\n",
    "    for word in words:\n",
    "        if word in synonims:\n",
    "            add += synonims[word]\n",
    "    words += add\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    words_info = [] # list of lists with docs numbers and number of word entries\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        if word in index.keys():\n",
    "            words_info.append(index[word])\n",
    "        else:\n",
    "            words_info.append([])\n",
    "    print(words_info)\n",
    "    # id of all docs which contain one of query's word\n",
    "    all_docs = list(set([doc_info[0] for word_info in words_info \n",
    "                         for doc_info in word_info]))\n",
    "    # word's counter for each document\n",
    "    docs_cnt = {el:0 for el in all_docs}\n",
    "    \n",
    "    # computing sum tf-idf for each document\n",
    "    for word_info in words_info:\n",
    "        if word_info:\n",
    "            idf = np.log(D / len(word_info))\n",
    "        else:\n",
    "            idf = 0\n",
    "        for doc_info in word_info: # doc_info = [doc_id, term_freq]\n",
    "            docs_cnt[doc_info[0]] += doc_info[1] * idf\n",
    "    # sort docs by it's frequency for query's words\n",
    "    docs_cnt = [item for item in docs_cnt.items()]\n",
    "    docs_cnt = sorted(docs_cnt, key=lambda x: x[1], reverse=True) # [(doc_id1, freq1), (doc_id2, freq2), ...]\n",
    "    doc_list = []\n",
    "    i = 0\n",
    "    for item in docs_cnt: # item[0] = doc_id\n",
    "        doc_list.append(item[0])\n",
    "        i += 1\n",
    "        if i == 10:\n",
    "            break\n",
    "    return (id, np.array(doc_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutex = Lock()\n",
    "n_processed = Value('i', 0)\n",
    "\n",
    "def func_wrapper(query_row):\n",
    "    result = get_prediction(query_row) \n",
    "    with mutex:\n",
    "        # в этом блоке можно безопасно менять общие объекты для процессов\n",
    "        global n_processed\n",
    "        n_processed.value += 1\n",
    "        if n_processed.value % 10 == 0:\n",
    "            print(f\"\\r{n_processed.value} objects are processed...\", end='', flush=True)\n",
    "    return result\n",
    "\n",
    "with Pool(processes=12) as pool:\n",
    "    result = pool.map(func_wrapper, queries.head().iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sorted(result, key=lambda x: x[0])\n",
    "\n",
    "docsId = []\n",
    "for id, docs in result:\n",
    "    docsId += docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({\n",
    "    'QueryId' : QueryId,\n",
    "    'DocumentId' : docsId\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('result.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
